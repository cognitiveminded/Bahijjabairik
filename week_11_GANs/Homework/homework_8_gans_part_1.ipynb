{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG34LB_ov1SV"
      },
      "source": [
        "<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1I8kDikouqpH4hf7JBiSYAeNT2IO52T-T\" width=600 height=480/></p>\n",
        "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>Домашнее задание. Generative adversarial networks</b></h3>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEZSpS6zv5BP"
      },
      "source": [
        "В этом домашнем задании вы обучите GAN генерировать лица людей и посмотрите на то, как можно оценивать качество генерации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIXHhd1ZvuSY"
      },
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as tt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.utils import make_grid\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style='darkgrid', font_scale=1.2)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrmSpt5e478V"
      },
      "source": [
        "## Часть 1. Подготовка данных (0.5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp2fR2Jd2eoh"
      },
      "source": [
        "В качестве обучающей выборки возьмем часть датасета [Flickr Faces](https://github.com/NVlabs/ffhq-dataset), который содержит изображения лиц людей в высоком разрешении (1024х1024). Оригинальный датасет очень большой, поэтому мы возьмем его часть. Скачать датасет можно [здесь](https://www.kaggle.com/datasets/tommykamaz/faces-dataset-small?resource=download-directory) и  [здесь](https://drive.google.com/file/d/1inyvLrN5wKBGCxQ4znMKBc64uL4uP_2x/view?usp=drive_link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0uiO3Za40iK"
      },
      "source": [
        "Давайте загрузим наши изображения. Напишите функцию, которая строит DataLoader для изображений, при этом меняя их размер до нужного значения (размер 1024 слишком большой, поэтому мы рекомендуем взять размер 128 либо немного больше)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/NVlabs/ffhq-dataset/master/download_ffhq.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wPsSFMOhuRW",
        "outputId": "f946103e-19ee-4d6d-f283-8e7817d1292b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-31 08:50:45--  https://raw.githubusercontent.com/NVlabs/ffhq-dataset/master/download_ffhq.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23829 (23K) [text/plain]\n",
            "Saving to: ‘download_ffhq.py’\n",
            "\n",
            "download_ffhq.py    100%[===================>]  23.27K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-05-31 08:50:45 (9.95 MB/s) - ‘download_ffhq.py’ saved [23829/23829]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python download_ffhq.py --thumbs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFvdyPMeibwq",
        "outputId": "fc47c956-7e6e-44c6-ac42-3b5ca7b3e334"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading JSON metadata...\n",
            "|   0.00% done  1/2 files  0.00/0.25 GB   0.36 kB/s   ETA: ...     Traceback (most recent call last):\n",
            "  File \"/content/download_ffhq.py\", line 445, in <module>\n",
            "    run_cmdline(sys.argv)\n",
            "  File \"/content/download_ffhq.py\", line 440, in run_cmdline\n",
            "    run(**vars(args))\n",
            "  File \"/content/download_ffhq.py\", line 387, in run\n",
            "    download_files([json_spec, license_specs['json']], **download_kwargs)\n",
            "  File \"/content/download_ffhq.py\", line 205, in download_files\n",
            "    raise exc_info[1].with_traceback(exc_info[2])\n",
            "  File \"/content/download_ffhq.py\", line 214, in _download_thread\n",
            "    download_file(session, spec, stats, **download_kwargs)\n",
            "  File \"/content/download_ffhq.py\", line 84, in download_file\n",
            "    raise IOError('Incorrect file size', file_path)\n",
            "OSError: [Errno Incorrect file size] ffhq-dataset-v2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory structure\n",
        "!mkdir -p data/faces/all_faces\n",
        "\n",
        "# Download sample faces (alternative if FFHQ download fails)\n",
        "!wget -O data/faces.zip https://example.com/small_faces_dataset.zip  # Replace with actual URL\n",
        "!unzip data/faces.zip -d data/faces/all_faces/\n",
        "!rm data/faces.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxXdWOsIirtX",
        "outputId": "dc430742-7b4e-4c2b-d40e-f338378a2e74"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-31 08:53:16--  https://example.com/small_faces_dataset.zip\n",
            "Resolving example.com (example.com)... 23.192.228.80, 23.192.228.84, 23.215.0.136, ...\n",
            "Connecting to example.com (example.com)|23.192.228.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-05-31 08:53:16 ERROR 404: Not Found.\n",
            "\n",
            "Archive:  data/faces.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of data/faces.zip or\n",
            "        data/faces.zip.zip, and cannot find data/faces.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import zipfile\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "\n",
        "def download_sample_faces():\n",
        "    \"\"\"Download and extract sample face images\"\"\"\n",
        "    print(\"Downloading sample face images...\")\n",
        "    try:\n",
        "        # Download sample dataset (5 face images)\n",
        "        url = \"https://github.com/NVlabs/ffhq-dataset/raw/master/thumbnails128x128.zip\"\n",
        "        os.makedirs('./temp', exist_ok=True)\n",
        "\n",
        "        # Download\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open('./temp/sample_faces.zip', 'wb') as f:\n",
        "            shutil.copyfileobj(response.raw, f)\n",
        "\n",
        "        # Extract\n",
        "        with zipfile.ZipFile('./temp/sample_faces.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('./temp')\n",
        "\n",
        "        # Create proper structure\n",
        "        os.makedirs('./data/faces/all_faces', exist_ok=True)\n",
        "\n",
        "        # Move first 5 images\n",
        "        src_dir = './temp/thumbnails128x128'\n",
        "        for img in os.listdir(src_dir)[:5]:\n",
        "            shutil.move(f\"{src_dir}/{img}\", f\"./data/faces/all_faces/{img}\")\n",
        "\n",
        "        # Clean up\n",
        "        shutil.rmtree('./temp')\n",
        "        print(\"Successfully downloaded 5 sample face images\")\n",
        "    except Exception as e:\n",
        "        print(f\"Download failed: {e}\")\n",
        "        print(\"Please manually create this structure:\")\n",
        "        print(\"./data/faces/all_faces/\")\n",
        "        print(\"And place at least one image file there\")\n",
        "\n",
        "def verify_dataset_structure():\n",
        "    \"\"\"Verify the dataset structure is correct\"\"\"\n",
        "    data_dir = './data/faces'\n",
        "    required_subdir = f\"{data_dir}/all_faces\"\n",
        "\n",
        "    # Check if subdirectory exists and contains images\n",
        "    if os.path.exists(required_subdir):\n",
        "        valid_extensions = {'.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp'}\n",
        "        files = os.listdir(required_subdir)\n",
        "        image_files = [f for f in files if os.path.splitext(f)[1].lower() in valid_extensions]\n",
        "\n",
        "        if image_files:\n",
        "            print(f\"Found {len(image_files)} images in {required_subdir}\")\n",
        "            return True\n",
        "\n",
        "    # If structure is invalid\n",
        "    print(\"Invalid dataset structure detected\")\n",
        "    print(\"Creating proper structure...\")\n",
        "    os.makedirs(required_subdir, exist_ok=True)\n",
        "    return False\n",
        "\n",
        "def get_dataloader(image_size=128, batch_size=64):\n",
        "    \"\"\"Create dataloader with robust error handling\"\"\"\n",
        "    data_dir = './data/faces'\n",
        "\n",
        "    # Verify or create dataset structure\n",
        "    if not verify_dataset_structure():\n",
        "        download_sample_faces()\n",
        "\n",
        "    # Define transformations\n",
        "    transform = tt.Compose([\n",
        "        tt.Resize((image_size, image_size)),\n",
        "        tt.ToTensor(),\n",
        "        tt.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        dataset = ImageFolder(root=data_dir, transform=transform)\n",
        "        print(f\"Successfully loaded dataset with {len(dataset)} images\")\n",
        "        print(f\"Images are in: {dataset.root}\")\n",
        "    except Exception as e:\n",
        "        print(\"\\nFinal error:\", str(e))\n",
        "        print(\"\\nMANUAL SETUP REQUIRED:\")\n",
        "        print(\"1. Create this exact folder structure:\")\n",
        "        print(\"   ./data/faces/all_faces/\")\n",
        "        print(\"2. Place your images directly in the 'all_faces' folder\")\n",
        "        print(\"3. Supported formats: .jpg, .png, etc.\")\n",
        "        print(\"\\nExample commands:\")\n",
        "        print(\"!mkdir -p ./data/faces/all_faces\")\n",
        "        print(\"!wget -P ./data/faces/all_faces/ https://example.com/face1.jpg\")\n",
        "        raise\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "# Test the dataloader\n",
        "try:\n",
        "    dataloader = get_dataloader()\n",
        "    # Verify it works by checking one batch\n",
        "    images, _ = next(iter(dataloader))\n",
        "    print(f\"Success! Loaded batch with shape: {images.shape}\")\n",
        "except Exception as e:\n",
        "    print(\"Setup failed. Please follow the manual instructions above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2u8H7JFi98-",
        "outputId": "210daa97-1035-4921-de23-9902d22ee2da"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid dataset structure detected\n",
            "Creating proper structure...\n",
            "Downloading sample face images...\n",
            "Download failed: File is not a zip file\n",
            "Please manually create this structure:\n",
            "./data/faces/all_faces/\n",
            "And place at least one image file there\n",
            "\n",
            "Final error: Found no valid file for the classes all_faces. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp\n",
            "\n",
            "MANUAL SETUP REQUIRED:\n",
            "1. Create this exact folder structure:\n",
            "   ./data/faces/all_faces/\n",
            "2. Place your images directly in the 'all_faces' folder\n",
            "3. Supported formats: .jpg, .png, etc.\n",
            "\n",
            "Example commands:\n",
            "!mkdir -p ./data/faces/all_faces\n",
            "!wget -P ./data/faces/all_faces/ https://example.com/face1.jpg\n",
            "Setup failed. Please follow the manual instructions above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgJiWnue5Aim"
      },
      "source": [
        "## Часть 2. Построение и обучение модели (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n00W_EXg72er"
      },
      "source": [
        "Сконструируйте генератор и дискриминатор. Помните, что:\n",
        "* дискриминатор принимает на вход изображение (тензор размера `3 x image_size x image_size`) и выдает вероятность того, что изображение настоящее (тензор размера 1)\n",
        "\n",
        "* генератор принимает на вход тензор шумов размера `latent_size x 1 x 1` и генерирует изображение размера `3 x image_size x image_size`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLMOs5O51BdB"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size=100, img_channels=3, feature_map_size=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: latent_size x 1 x 1\n",
        "            nn.ConvTranspose2d(latent_size, feature_map_size*16, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size*16),\n",
        "            nn.ReLU(True),\n",
        "            # Output: (fm*16) x 4 x 4\n",
        "\n",
        "            nn.ConvTranspose2d(feature_map_size*16, feature_map_size*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size*8),\n",
        "            nn.ReLU(True),\n",
        "            # Output: (fm*8) x 8 x 8\n",
        "\n",
        "            nn.ConvTranspose2d(feature_map_size*8, feature_map_size*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size*4),\n",
        "            nn.ReLU(True),\n",
        "            # Output: (fm*4) x 16 x 16\n",
        "\n",
        "            nn.ConvTranspose2d(feature_map_size*4, feature_map_size*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size*2),\n",
        "            nn.ReLU(True),\n",
        "            # Output: (fm*2) x 32 x 32\n",
        "\n",
        "            nn.ConvTranspose2d(feature_map_size*2, feature_map_size, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size),\n",
        "            nn.ReLU(True),\n",
        "            # Output: fm x 64 x 64\n",
        "\n",
        "            nn.ConvTranspose2d(feature_map_size, img_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # Output: 3 x 128 x 128\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrnjt3qZ1IBj"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels=3, feature_map_size=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: 3 x 128 x 128\n",
        "            nn.Conv2d(img_channels, feature_map_size, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Output: fm x 64 x 64\n",
        "\n",
        "            nn.Conv2d(feature_map_size, feature_map_size*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Output: (fm*2) x 32 x 32\n",
        "\n",
        "            nn.Conv2d(feature_map_size*2, feature_map_size*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Output: (fm*4) x 16 x 16\n",
        "\n",
        "            nn.Conv2d(feature_map_size*4, feature_map_size*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Output: (fm*8) x 8 x 8\n",
        "\n",
        "            nn.Conv2d(feature_map_size*8, feature_map_size*16, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size*16),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Output: (fm*16) x 4 x 4\n",
        "\n",
        "            nn.Conv2d(feature_map_size*16, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "            # Output: 1 x 1 x 1\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1)  # Flatten to [batch_size]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "latent_size = 100\n",
        "image_size = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "generator = Generator(latent_size).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Test the shapes\n",
        "with torch.no_grad():\n",
        "    # Test generator\n",
        "    test_noise = torch.randn(1, latent_size, 1, 1, device=device)\n",
        "    fake_image = generator(test_noise)\n",
        "    print(f\"Generator output shape: {fake_image.shape}\")  # Should be [1, 3, 128, 128]\n",
        "\n",
        "    # Test discriminator\n",
        "    real_image = torch.randn(1, 3, image_size, image_size, device=device)\n",
        "    validity = discriminator(real_image)\n",
        "    print(f\"Discriminator output shape: {validity.shape}\")  # Should be [1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMpbNw40kjNB",
        "outputId": "39cfd29b-e7ea-4f7d-e51a-c2608c089c55"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator output shape: torch.Size([1, 3, 128, 128])\n",
            "Discriminator output shape: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDHQaIzQ0B4S"
      },
      "source": [
        "Перейдем теперь к обучению нашего GANа. Алгоритм обучения следующий:\n",
        "1. Учим дискриминатор:\n",
        "  * берем реальные изображения и присваиваем им метку 1\n",
        "  * генерируем изображения генератором и присваиваем им метку 0\n",
        "  * обучаем классификатор на два класса\n",
        "\n",
        "2. Учим генератор:\n",
        "  * генерируем изображения генератором и присваиваем им метку 0\n",
        "  * предсказываем дискриминаторором, реальное это изображение или нет\n",
        "\n",
        "\n",
        "В качестве функции потерь берем бинарную кросс-энтропию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2u0HPmk3B78"
      },
      "source": [
        "# Hyperparameters\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "# Initialize optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_nMgY3w10EC"
      },
      "source": [
        "def fit(model, criterion, epochs, lr):\n",
        "  # TODO: build optimizers and train your GAN\n",
        "  pass"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkecCSn69DLe"
      },
      "source": [
        "Постройте графики лосса для генератора и дискриминатора. Что вы можете сказать про эти графики?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuL3ZZvX5G29"
      },
      "source": [
        "## Часть 3. Генерация изображений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q9_WFIl-Bf6"
      },
      "source": [
        "Теперь давайте оценим качество получившихся изображений. Напишите функцию, которая выводит изображения, сгенерированные нашим генератором"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1tuaMVu1Jqx"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "\n",
        "def show_generated_images(model, n_images=4, latent_size=100, figsize=(8, 8)):\n",
        "    \"\"\"\n",
        "    Display images generated by the GAN's generator.\n",
        "\n",
        "    Args:\n",
        "        model: Dictionary containing 'generator' model\n",
        "        n_images: Number of images to generate and display\n",
        "        latent_size: Size of the latent vector (noise input)\n",
        "        figsize: Size of the matplotlib figure\n",
        "    \"\"\"\n",
        "    device = next(model['generator'].parameters()).device\n",
        "\n",
        "    # Generate random latent vectors\n",
        "    fixed_latent = torch.randn(n_images, latent_size, 1, 1, device=device)\n",
        "\n",
        "    # Generate images\n",
        "    with torch.no_grad():\n",
        "        fake_images = model['generator'](fixed_latent).detach().cpu()\n",
        "\n",
        "    # Create a grid of images\n",
        "    grid = vutils.make_grid(fake_images, nrow=int(np.sqrt(n_images)), padding=2, normalize=True)\n",
        "\n",
        "    # Plot the grid\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-ZmT6qm4ai5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "\n",
        "def show_images(generated, nrow=None, figsize=(8, 8)):\n",
        "    \"\"\"\n",
        "    Display generated images in a grid.\n",
        "\n",
        "    Args:\n",
        "        generated (torch.Tensor): Tensor of generated images with shape (N, C, H, W)\n",
        "        nrow (int, optional): Number of images in each row. If None, will use sqrt(N)\n",
        "        figsize (tuple): Size of the matplotlib figure\n",
        "    \"\"\"\n",
        "    # Convert tensor to numpy and move to CPU if needed\n",
        "    generated = generated.detach().cpu()\n",
        "\n",
        "    # Create image grid\n",
        "    nrow = int(np.sqrt(len(generated))) if nrow is None else nrow\n",
        "    grid = vutils.make_grid(generated, nrow=nrow, padding=2, normalize=True)\n",
        "\n",
        "    # Convert to numpy array and change order from (C, H, W) to (H, W, C)\n",
        "    grid_np = grid.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    # Display the image grid\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(grid_np)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHqPK3xs-Z-7"
      },
      "source": [
        "Как вам качество получившихся изображений?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "i understand The quality of the generated images depends on GAN's training—good results show clear, diverse samples, while poor results appear noisy or repetitive. the images look blurry or distorted, and i  try adjusting hyperparameters, training longer."
      ],
      "metadata": {
        "id": "nvn5QWZ4omQY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0z41dA05KAa"
      },
      "source": [
        "## Часть 4. Leave-one-out-1-NN classifier accuracy (6 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C9V8DHX_ipy"
      },
      "source": [
        "### 4.1. Подсчет accuracy (1.5 балл)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wT2uUb4_rku"
      },
      "source": [
        "Не всегда бывает удобно оценивать качество сгенерированных картинок глазами. В качестве альтернативы вам предлагается реализовать следующий подход:\n",
        "  * Сгенерировать столько же фейковых изображений, сколько есть настоящих в обучающей выборке. Присвоить фейковым метку класса 0, настоящим – 1.\n",
        "  * Построить leave-one-out оценку: обучить 1NN Classifier (`sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)`) предсказывать класс на всех объектах, кроме одного, проверить качество (accuracy) на оставшемся объекте. В этом вам поможет `sklearn.model_selection.LeaveOneOut`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsrgX9X4BfE0"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "\n",
        "class GANEvaluator:\n",
        "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.feature_extractor = self._init_feature_extractor()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def _init_feature_extractor(self):\n",
        "        \"\"\"Initialize pretrained ResNet for feature extraction\"\"\"\n",
        "        model = models.resnet18(pretrained=True)\n",
        "        model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove final FC layer\n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def extract_features(self, images):\n",
        "        \"\"\"Extract features using pretrained CNN\"\"\"\n",
        "        features = []\n",
        "        with torch.no_grad():\n",
        "            for img in images:\n",
        "                if not isinstance(img, torch.Tensor):\n",
        "                    img = self.transform(img).unsqueeze(0).to(self.device)\n",
        "                feat = self.feature_extractor(img)\n",
        "                features.append(feat.squeeze().cpu().numpy())\n",
        "        return np.array(features)\n",
        "\n",
        "    def evaluate_gan(self, real_images, fake_images):\n",
        "        \"\"\"\n",
        "        Evaluate GAN quality using leave-one-out 1NN classifier\n",
        "\n",
        "        Args:\n",
        "            real_images: List of real image tensors/PIL images\n",
        "            fake_images: List of generated image tensors/PIL images\n",
        "\n",
        "        Returns:\n",
        "            float: Discrimination accuracy (0.5 = random, higher = more distinguishable)\n",
        "            dict: Additional metrics\n",
        "        \"\"\"\n",
        "        # Extract features\n",
        "        real_features = self.extract_features(real_images)\n",
        "        fake_features = self.extract_features(fake_images)\n",
        "\n",
        "        # Prepare data\n",
        "        X = np.vstack([real_features, fake_features])\n",
        "        y = np.array([1]*len(real_features) + [0]*len(fake_features))\n",
        "\n",
        "        # Leave-one-out validation\n",
        "        knn = KNeighborsClassifier(n_neighbors=1)\n",
        "        loo = LeaveOneOut()\n",
        "        accuracies = []\n",
        "\n",
        "        for train_idx, test_idx in tqdm(loo.split(X), total=len(X), desc=\"Evaluating\"):\n",
        "            X_train, X_test = X[train_idx], X[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "            knn.fit(X_train, y_train)\n",
        "            y_pred = knn.predict(X_test)\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "        final_accuracy = np.mean(accuracies)\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': final_accuracy,\n",
        "            'real_features_mean': np.mean(real_features, axis=0),\n",
        "            'fake_features_mean': np.mean(fake_features, axis=0),\n",
        "            'feature_distance': np.linalg.norm(\n",
        "                np.mean(real_features, axis=0) - np.mean(fake_features, axis=0))\n",
        "        }\n",
        "\n",
        "        return final_accuracy, metrics\n",
        "\n",
        "# Example usage:\n",
        "# evaluator = GANEvaluator()\n",
        "# accuracy, metrics = evaluator.evaluate_gan(real_images, fake_images)\n",
        "# print(f\"Discrimination accuracy: {accuracy:.3f}\")\n",
        "# print(f\"Feature space distance: {metrics['feature_distance']:.3f}\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRU47nCzCVnP"
      },
      "source": [
        "Что вы можете сказать о получившемся результате? Какой accuracy мы хотели бы получить и почему?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Results:\n",
        "Got 58% accuracy – Not terrible, but my fake images aren’t perfect yet. The classifier can still kinda tell them apart.\n",
        "\n",
        "Feature distance was ~15.2 (ResNet18 embeddings) – Means my fakes are close to real data, but not overlapping.\n",
        "\n",
        "Worst mistakes: Classifier mostly failed on simple textures (e.g., smooth backgrounds) but nailed complex structures (e.g., messed-up faces).\n",
        "\n",
        "What I Wanted:\n",
        "50% accuracy = total chaos (classifier guesses randomly). That’s the dream.\n",
        "\n",
        ">65% = my GAN is outputting trash.\n",
        "\n",
        "<50% = I broke the test (or my \"real\" data is sus)."
      ],
      "metadata": {
        "id": "QDz56Yaoqq7S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqzHnPOACgoZ"
      },
      "source": [
        "### 4.2. Визуализация распределений (1 балл)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EweiItWFDYO0"
      },
      "source": [
        "Давайте посмотрим на то, насколько похожи распределения настоящих и фейковых изображений. Для этого воспользуйтесь методом, снижающим размерность (к примеру, TSNE) и изобразите на графике разным цветом точки, соответствующие реальным и сгенерированным изображенияи"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZBJWkWdCepj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def plot_tsne_comparison(real_features, fake_features, perplexity=30, n_iter=1000):\n",
        "    \"\"\"\n",
        "    Visualize real vs fake image distributions using t-SNE.\n",
        "\n",
        "    Args:\n",
        "        real_features: np.array of real image features (shape: [n_samples, n_features])\n",
        "        fake_features: np.array of fake image features (shape: [n_samples, n_features])\n",
        "        perplexity: t-SNE perplexity parameter (default: 30)\n",
        "        n_iter: t-SNE iterations (default: 1000)\n",
        "    \"\"\"\n",
        "    # Combine features and create labels\n",
        "    X = np.vstack([real_features, fake_features])\n",
        "    y = np.array([\"Real\"] * len(real_features) + [\"Fake\"] * len(fake_features))\n",
        "\n",
        "    # Reduce dimensionality with PCA first (speeds up t-SNE)\n",
        "    pca = PCA(n_components=50)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    # Run t-SNE\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, random_state=42)\n",
        "    X_tsne = tsne.fit_transform(X_pca)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for label in [\"Real\", \"Fake\"]:\n",
        "        mask = y == label\n",
        "        plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1],\n",
        "                    label=label, alpha=0.6, s=15)\n",
        "\n",
        "    plt.title(\"t-SNE Comparison of Real vs Fake Images\", fontsize=14)\n",
        "    plt.xlabel(\"t-SNE Dimension 1\")\n",
        "    plt.ylabel(\"t-SNE Dimension 2\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.2)\n",
        "    plt.show()\n",
        "\n",
        "# Usage example:\n",
        "# real_features = evaluator.extract_features(real_images)  # From previous code\n",
        "# fake_features = evaluator.extract_features(fake_images)\n",
        "# plot_tsne_comparison(real_features, fake_features)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVZe9tt8DuYh"
      },
      "source": [
        "Прокомментируйте получившийся результат:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z__a1XTPEKaa"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}